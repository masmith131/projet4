{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 0 : import \n",
    "\n",
    "from os import walk \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib.pyplot import imshow, subplots, title, xticks, legend, show, figure\n",
    "from numpy import zeros, arange, where\n",
    "from keras import Sequential\n",
    "from keras.layers import RandomZoom, RandomRotation, Conv2D, MaxPool2D, Dropout, Flatten, Dense\n",
    "from random import randint\n",
    "from tensorflow import gather  \n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First part : training the model \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# CELL 1.1 : accessing to data \n",
    "# Needs [ CELL 0 : import ]\n",
    "\n",
    "# defines variables \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "path_training = 'BelgiumTSC_Training\\Training'\n",
    "path_testing = 'BelgiumTSC_Testing\\Testing'\n",
    "\n",
    "nbr_class = 62 # len(next(walk(path_training))[1])  counting the number of classes \n",
    "print('number of classes : ', nbr_class)\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# resol is the format for the images that we want for convenience\n",
    "\n",
    "resol = (50,50) # Square image only ! (data augmentation flip)\n",
    "print(\"Resolution of images : \", resol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.2 : accessing to data \n",
    "# Needs [ CELL 1.1 ]\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# retrieving the number of images to be treated in the training folder\n",
    "\n",
    "DIR = path_training\n",
    "counter = 0\n",
    "for root, dirs, files in walk(DIR) :\n",
    "    for file in files:    \n",
    "        if file.endswith('.ppm') or file.endswith('.jpeg'):\n",
    "            counter += 1\n",
    "\n",
    "print(\"number of images in the training folder : \", counter)\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Creating an image.DirectoryIterator to work over the images of the training folder  \n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "training_set = train_datagen.flow_from_directory(path_training,target_size = resol,\n",
    "batch_size = counter,class_mode = 'binary', color_mode='rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.3 : accessing to data \n",
    "# Needs [ CELL 1.1 ]\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# retrieving the number of images to be treated in the testing folder\n",
    "\n",
    "DIR = path_testing\n",
    "counter = 0\n",
    "for root, dirs, files in walk(DIR) :\n",
    "    for file in files:    \n",
    "        if file.endswith('.ppm') or file.endswith('.jpeg'):\n",
    "            counter += 1\n",
    "\n",
    "print(\"number of images in the testing folder : \", counter)\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Creating an image.DirectoryIterator to work over the images of the testing folder \n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_set = test_datagen.flow_from_directory(path_testing,target_size = resol,\n",
    "batch_size = counter,class_mode = 'binary', color_mode='rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.4 : storing in arrays\n",
    "# Needs [ CELL 1.2, CELL 1.3 ]\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Storing all the information in arrays for convenience \n",
    "\n",
    "X_train , y_train = training_set.next()\n",
    "X_test , y_test = test_set.next()\n",
    "\n",
    "print(\"Shape of X_train : \", X_train.shape)\n",
    "print(\"Shape of y_train : \", y_train.shape)\n",
    "print(\"Shape of X_test : \", X_test.shape)\n",
    "print(\"Shape of y_test : \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.5 : visualisation of the initial sets\n",
    "# Needs [ CELL 1.4 ] \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Print the number of signs of each type in the initial sets\n",
    "\n",
    "initial_nbr_train = zeros(nbr_class, dtype=int)\n",
    "initial_nbr_test = zeros(nbr_class, dtype=int)\n",
    "\n",
    "for i in range(nbr_class) :\n",
    "    initial_nbr_train[i] = int((y_train.copy() == i).sum())  # Number of images of class i in the Training set\n",
    "    initial_nbr_test[i] = int((y_test.copy() == i).sum())   # Number of images of class i in the Test set \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "print(\"Number of each sign in the train set : \")\n",
    "print()\n",
    "print(initial_nbr_train)\n",
    "print()\n",
    "print(\"Total of signs : \", initial_nbr_train.sum())\n",
    "print()\n",
    "\n",
    "print(\"Number of each sign in the test set : \")\n",
    "print()\n",
    "print(initial_nbr_test)\n",
    "print()\n",
    "print(\"Total of signs : \", initial_nbr_test.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.6 :  visualisation of the dataset\n",
    "# Needs [ CELL 1.5 ]\n",
    "\n",
    "# Defines a fct that plot histo of what's in the set \n",
    "# Also print the same info as CELL 1.5 \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "def graphs (nrb_train, y_train) : \n",
    "\n",
    "    print(\"Number of each sign in the train set : \")\n",
    "    print()\n",
    "    print(nrb_train)\n",
    "    print()\n",
    "    print(\"Total of signs : \", nrb_train.sum())\n",
    "    print()\n",
    "\n",
    "    print(\"Number of each sign in the test set : \")\n",
    "    print()\n",
    "    print(initial_nbr_test)\n",
    "    print()\n",
    "    print(\"Total of signs : \", initial_nbr_test.sum())\n",
    "\n",
    "    #-----------------------------------------------------------------------------\n",
    "\n",
    "    # We plot an histo showing how many signs of each class we have in each set \n",
    "\n",
    "    fig, ax = subplots(figsize = (20, 7))\n",
    "    bins = [x + 0.5 for x in range(-1, nbr_class)]\n",
    "    ax.hist([y_train.copy(), y_test.copy()], range = (0, nbr_class-1), bins=bins, edgecolor = 'white', color = ['blueviolet','black'], label = ['y_train', 'y_test'])\n",
    "    title(\"Visualisation of the number of signs of each class in each set\")\n",
    "    xticks(arange(nbr_class))\n",
    "    legend()\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.7 :  visualisation of the dataset\n",
    "# Needs [ CELL 1.6 :  visualisation of the dataset ]\n",
    "\n",
    "# Visualisation of the initial sets \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "graphs(initial_nbr_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.8 : randomization for augmentation \n",
    "# Needs [ CELL 1.4 ]\n",
    "\n",
    "# This cell is about data augmentation \n",
    "# We'll randomize our initial set and do our augmentation with this (cells later)\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "data_augmentation = Sequential() \n",
    "\n",
    "data_augmentation.add(RandomZoom(0.05))\n",
    "data_augmentation.add(RandomRotation(0.05)) \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# We apply the augmentation on our datasets \n",
    "augmented_image_train = data_augmentation(X_train.copy())\n",
    "augmented_image_test = data_augmentation(X_test.copy())\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "print(\"Shape of the randomized test set : \", augmented_image_test.shape)\n",
    "print(\"Shape of the randomized training set : \", augmented_image_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.9 : test of the randomization \n",
    "# Needs [ CELL 1.8 ]\n",
    "\n",
    "# A little test to see the result of the augmentation \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "index = randint(0,len(X_train))\n",
    "figure()\n",
    "imshow(X_train[index])\n",
    "figure()\n",
    "imshow(augmented_image_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.10 : augmentation\n",
    "# Needs [ CELL 1.8 ] \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Second method of data augmentation \n",
    "# All classes are represented with the same number of sign, the max already in \n",
    "\n",
    "memory = zeros(nbr_class, dtype=int)\n",
    "lim = max(initial_nbr_train) #500\n",
    "counter = 0\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "for i in range(nbr_class) :\n",
    "\n",
    "    memory[i] = lim - initial_nbr_train[i]\n",
    "    counter += lim - initial_nbr_train[i]\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# We create new sets that we'll fill with the data of the initial sets + the augmented data\n",
    "\n",
    "X_train_second = zeros((len(X_train) + counter, resol[0], resol[1], 3))\n",
    "y_train_second = zeros(len(y_train) + counter)\n",
    "\n",
    "X_train_second[:len(X_train)] = X_train.copy()\n",
    "y_train_second[:len(y_train)] = y_train.copy()\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# We'll start adding values at this index\n",
    "\n",
    "index = len(X_train)\n",
    "\n",
    "nbr_train_second = initial_nbr_train.copy() \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "for i in range(nbr_class) :\n",
    "\n",
    "    indices = where(y_train.copy() == i)\n",
    "    augmented_image = gather(X_train.copy(), indices=indices[0])\n",
    "\n",
    "    for j in range(memory[i]) :\n",
    "\n",
    "        idx = randint(0, len(indices[0])-1)\n",
    "\n",
    "        nbr_train_second[i] += 1 \n",
    "\n",
    "        X_train_second[index] = augmented_image[idx]\n",
    "        y_train_second[index] = i\n",
    "        index += 1\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "print(\"Shape of the augmented training set with second method : \", X_train_second.shape)\n",
    "print(\"Shape of the augmented training target with second method : \", y_train_second.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.11 :  visualisation of the dataset\n",
    "# Needs [ CELL 1.10 ]\n",
    "\n",
    "# Visualisation of the augmented sets \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "graphs(nbr_train_second,y_train_second )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.12 : categorical\n",
    "# Needs [ CELL 1.10 ]\n",
    "\n",
    "# We put our results to categorical\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "y_test_tc = to_categorical(y_test.copy(), nbr_class)\n",
    "y_train_tc = to_categorical(y_train.copy(), nbr_class)\n",
    "\n",
    "print(\"Shape of y_train without augmentation : \", y_train_tc.shape)\n",
    "print(\"Shape of y_test without augmentation : \", y_test_tc.shape)\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "y_train_second_tc = to_categorical(y_train_second.copy(), nbr_class)\n",
    "\n",
    "print(\"Shape of y_train with second method of augmentation : \", y_train_second_tc.shape)\n",
    "print(\"Shape of y_test with second method of augmentation : \", y_test_tc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.13 : construction of the model \n",
    "# Needs [ CELL 0 ]\n",
    "\n",
    "# We construct our model\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "def construct_model (array) : \n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Tune the number of filters for the second Conv2D \n",
    "    # Choose an optimal value from 64-128\n",
    "    \n",
    "    model.add(Conv2D(kernel_size=(6,6),filters=112, activation='relu', input_shape=X_train_second.shape[1:]))\n",
    "    model.add(MaxPool2D(pool_size=(3,3)))\n",
    "    model.add(Dropout(rate=0.25))\n",
    "\n",
    "    model.add(Conv2D(kernel_size=(3,3),filters=208, activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(3,3)))\n",
    "    model.add(Dropout(rate=0.25))\n",
    "    \n",
    "    model.add(Conv2D(kernel_size=(2,2),filters=256, activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(rate=0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(82, activation = 'relu'))\n",
    "    model.add(Dense(nbr_class, activation = 'softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.14 : construction of the model \n",
    "# Needs [ CELL 1.13, CELL 1.10 ]\n",
    "\n",
    "# We construct our model based on the augmented dataset \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "model_second = construct_model(X_train_second.copy())\n",
    "model_second.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# We train here the model with the first method of augmentation \n",
    "\n",
    "eps = 12 # The number of time we want the model to train on the entire training_set\n",
    "# steps = 300 # the number of iteration for each epochs \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "validation_X_second = X_test.copy()\n",
    "validation_y_second = y_test_tc.copy()\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "mycallbacks = [EarlyStopping(restore_best_weights=True, patience=4), ReduceLROnPlateau(patience=4)] \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "anc_second = model_second.fit(X_train_second.copy(),\n",
    "                              y_train_second_tc.copy(),\n",
    "                              validation_data=(validation_X_second,validation_y_second),\n",
    "                              epochs=eps,\n",
    "                              # steps_per_epoch=steps, \n",
    "                              callbacks = mycallbacks)\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "#added aug.flow to (X_train, y_train) to do data augmentation\n",
    "# validation_data=(X_test_new_first, y_test_new_first)\n",
    "# validation_data=(X_test, y_test)\n",
    "\n",
    "# talk : aug.flow ? \n",
    "# talk : validation data strange "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure, subplot, plot, title, xlabel, ylabel, legend, tight_layout, show\n",
    "\n",
    "def perf(anc) : \n",
    "\n",
    "    # Plot of performances \n",
    "\n",
    "    figure()\n",
    "\n",
    "    subplot(2, 1, 1)\n",
    "    plot(anc.history['accuracy'], label='training accuracy', color = 'darkblue')\n",
    "    plot(anc.history['val_accuracy'], label='test accuracy', color = 'magenta')\n",
    "    title('Accuracy')\n",
    "    xlabel('epochs')\n",
    "    ylabel('accuracy')\n",
    "    legend()\n",
    "\n",
    "    subplot(2, 1, 2)\n",
    "    plot(anc.history['loss'], label='training loss', color = 'darkblue')\n",
    "    plot(anc.history['val_loss'], label='test loss', color = 'magenta')\n",
    "    title('Loss')\n",
    "    xlabel('epochs')\n",
    "    ylabel('loss')\n",
    "    legend()\n",
    "\n",
    "    tight_layout()\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf(anc_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a function that will give the score that we can see on kaggle \n",
    "\n",
    "true = y_test_tc.copy().argmax(axis=1)\n",
    "\n",
    "print(\"True codes : \", true)\n",
    "print(\"Number of true codes : \", len(true))\n",
    "\n",
    "predict = model_second.predict(X_test.copy()).argmax(axis=1)\n",
    "\n",
    "print(\"Predictions : \", predict)\n",
    "print(\"Number of predictions : \", len(predict))\n",
    "\n",
    "right = 0 \n",
    "\n",
    "for i in range(len(true)) : \n",
    "    if predict[i] == true[i] :  \n",
    "        right += 1 \n",
    "\n",
    "print(\"Number of right : \", right)\n",
    "print(\"Number of elements : \", len(true))\n",
    "\n",
    "print(\"Ratio : \", right/len(true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = open(\"dict.csv\").read().strip().split(\"\\n\")[0:]\n",
    "label_names = [l.split(\",\")[1] for l in label_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate the network\n",
    "\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions_second = model_second.predict(X_test.copy()) \n",
    "print(classification_report(y_test_tc.copy().argmax(axis=1),\n",
    "\tpredictions_second.argmax(axis=1), target_names=label_names, labels=range(nbr_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from numpy import asarray, append, array\n",
    "from PIL import Image\n",
    "\n",
    "# We store all of the images from the kaggle folder in an array \n",
    "# recall that target has been defined above \n",
    "\n",
    "images = [] \n",
    "names = []\n",
    "\n",
    "# get the path/directory\n",
    "folder_dir = 'eval_kaggle1'\n",
    "\n",
    "for image in listdir(folder_dir):\n",
    "    # check if the image ends with ppm\n",
    "    if (image.endswith(\".ppm\")):\n",
    "        img = Image.open(folder_dir + '/' + image)\n",
    "        img = img.resize(resol) # (30,30) as an example \n",
    "        img = asarray(img)\n",
    "        images.append(img) \n",
    "        names.append(image.replace('.ppm',''))\n",
    "\n",
    "images = array(images)\n",
    "\n",
    "print(\"Number of images and their resolution in the kaggle dataset : \", images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a code to save all ppm in jpeg in a directory called names (must be created)\n",
    "\n",
    "# get the path/directory\n",
    "folder_dir = \"eval_kaggle1\"\n",
    "\n",
    "for image in listdir(folder_dir):\n",
    "    # check if the image ends with ppm\n",
    "    if (image.endswith(\".ppm\")):\n",
    "        img = Image.open(folder_dir + '/' + image)\n",
    "        img.save(\"names\" + '/' + image.replace('.ppm','.jpg'), format = 'JPEG') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import DictReader\n",
    "\n",
    "# We will store the data in dict.csv in a dict \n",
    "\n",
    "data = {}\n",
    "\n",
    "with open('dict.csv', 'r') as f:\n",
    "    d_reader = DictReader(f, fieldnames=[\"num\", \"sign\"])\n",
    "\n",
    "    #get fieldnames from DictReader object and store in list\n",
    "    for row in d_reader:\n",
    "        data[row['num']] = row['sign']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure, show, title, imshow \n",
    "from csv import DictWriter\n",
    "\n",
    "# We then print the image with num and the sign predicted as a title \n",
    "\n",
    "predictions = model_second.predict(images).argmax(axis=1) \n",
    "\n",
    "for i in range(20): \n",
    "    figure(figsize = (10,10))\n",
    "    imshow(images[i])\n",
    "    sign = data[str(predictions[i])]\n",
    "    title(str(predictions[i]) + \" : \" + str(sign))\n",
    "\n",
    "# Here is the code to write the results in a CSV for kaggle \n",
    "\n",
    "with open('final_' + '.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['Id', 'Category']\n",
    "    writer = DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for i in range(len(predictions)): \n",
    "        writer.writerow({'Id' : names[i], 'Category' : predictions[i]}) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
