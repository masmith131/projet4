{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 0 : import \n",
    "\n",
    "from os import walk \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib.pyplot import imshow, subplots, title, xticks, legend, show, figure\n",
    "from numpy import zeros, arange, where\n",
    "from keras import Sequential\n",
    "from keras.layers import RandomZoom, RandomRotation, Conv2D, MaxPool2D, Dropout, Flatten, Dense\n",
    "from random import randint\n",
    "from tensorflow import gather  \n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.1 : accessing to data \n",
    "# Needs [ CELL 0 : import ]\n",
    "\n",
    "# defines variables \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "path_training = 'BelgiumTSC_Training\\Training'\n",
    "path_testing = 'BelgiumTSC_Testing\\Testing'\n",
    "\n",
    "nbr_class = 62 # len(next(walk(path_training))[1])  counting the number of classes \n",
    "print('number of classes : ', nbr_class)\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# resol is the format for the images that we want for convenience\n",
    "\n",
    "resol = (50,50) # Square image only ! (data augmentation flip)\n",
    "print(\"Resolution of images : \", resol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.2 : accessing to data \n",
    "# Needs [ CELL 1.1 ]\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# retrieving the number of images to be treated in the training folder\n",
    "\n",
    "DIR = path_training\n",
    "counter = 0\n",
    "for root, dirs, files in walk(DIR) :\n",
    "    for file in files:    \n",
    "        if file.endswith('.ppm') or file.endswith('.jpeg'):\n",
    "            counter += 1\n",
    "\n",
    "print(\"number of images in the training folder : \", counter)\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Creating an image.DirectoryIterator to work over the images of the training folder  \n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "training_set = train_datagen.flow_from_directory(path_training,target_size = resol,\n",
    "batch_size = counter,class_mode = 'binary', color_mode='rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.3 : accessing to data \n",
    "# Needs [ CELL 1.1 ]\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# retrieving the number of images to be treated in the testing folder\n",
    "\n",
    "DIR = path_testing\n",
    "counter = 0\n",
    "for root, dirs, files in walk(DIR) :\n",
    "    for file in files:    \n",
    "        if file.endswith('.ppm') or file.endswith('.jpeg'):\n",
    "            counter += 1\n",
    "\n",
    "print(\"number of images in the testing folder : \", counter)\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Creating an image.DirectoryIterator to work over the images of the testing folder \n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_set = test_datagen.flow_from_directory(path_testing,target_size = resol,\n",
    "batch_size = counter,class_mode = 'binary', color_mode='rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.4 : storing in arrays\n",
    "# Needs [ CELL 1.2, CELL 1.3 ]\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Storing all the information in arrays for convenience \n",
    "\n",
    "X_train , y_train = training_set.next()\n",
    "X_test , y_test = test_set.next()\n",
    "\n",
    "print(\"Shape of X_train : \", X_train.shape)\n",
    "print(\"Shape of y_train : \", y_train.shape)\n",
    "print(\"Shape of X_test : \", X_test.shape)\n",
    "print(\"Shape of y_test : \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.5 : visualisation of the initial sets\n",
    "# Needs [ CELL 1.4 ] \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Print the number of signs of each type in the initial sets\n",
    "\n",
    "initial_nbr_train = zeros(nbr_class, dtype=int)\n",
    "initial_nbr_test = zeros(nbr_class, dtype=int)\n",
    "\n",
    "for i in range(nbr_class) :\n",
    "    initial_nbr_train[i] = int((y_train.copy() == i).sum())  # Number of images of class i in the Training set\n",
    "    initial_nbr_test[i] = int((y_test.copy() == i).sum())   # Number of images of class i in the Test set \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "print(\"Number of each sign in the train set : \")\n",
    "print()\n",
    "print(initial_nbr_train)\n",
    "print()\n",
    "print(\"Total of signs : \", initial_nbr_train.sum())\n",
    "print()\n",
    "\n",
    "print(\"Number of each sign in the test set : \")\n",
    "print()\n",
    "print(initial_nbr_test)\n",
    "print()\n",
    "print(\"Total of signs : \", initial_nbr_test.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.6 :  visualisation of the dataset\n",
    "# Needs [ CELL 1.5 ]\n",
    "\n",
    "# Defines a fct that plot histo of what's in the set \n",
    "# Also print the same info as CELL 1.5 \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "def graphs (nrb_train, y_train) : \n",
    "\n",
    "    print(\"Number of each sign in the train set : \")\n",
    "    print()\n",
    "    print(nrb_train)\n",
    "    print()\n",
    "    print(\"Total of signs : \", nrb_train.sum())\n",
    "    print()\n",
    "\n",
    "    print(\"Number of each sign in the test set : \")\n",
    "    print()\n",
    "    print(initial_nbr_test)\n",
    "    print()\n",
    "    print(\"Total of signs : \", initial_nbr_test.sum())\n",
    "\n",
    "    #-----------------------------------------------------------------------------\n",
    "\n",
    "    # We plot an histo showing how many signs of each class we have in each set \n",
    "\n",
    "    fig, ax = subplots(figsize = (20, 7))\n",
    "    bins = [x + 0.5 for x in range(-1, nbr_class)]\n",
    "    ax.hist([y_train.copy(), y_test.copy()], range = (0, nbr_class-1), bins=bins, edgecolor = 'white', color = ['blueviolet','black'], label = ['y_train', 'y_test'])\n",
    "    title(\"Visualisation of the number of signs of each class in each set\")\n",
    "    xticks(arange(nbr_class))\n",
    "    legend()\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.7 :  visualisation of the dataset\n",
    "# Needs [ CELL 1.6 :  visualisation of the dataset ]\n",
    "\n",
    "# Visualisation of the initial sets \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "graphs(initial_nbr_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.8 : randomization for augmentation \n",
    "# Needs [ CELL 1.4 ]\n",
    "\n",
    "# This cell is about data augmentation \n",
    "# We'll randomize our initial set and do our augmentation with this (cells later)\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "data_augmentation = Sequential() \n",
    "\n",
    "data_augmentation.add(RandomZoom(0.2))\n",
    "data_augmentation.add(RandomRotation(0.1)) \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# We apply the augmentation on our datasets \n",
    "augmented_image_train = data_augmentation(X_train.copy())\n",
    "augmented_image_test = data_augmentation(X_test.copy())\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "print(\"Shape of the randomized test set : \", augmented_image_test.shape)\n",
    "print(\"Shape of the randomized training set : \", augmented_image_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.9 : test of the randomization \n",
    "# Needs [ CELL 1.8 ]\n",
    "\n",
    "# A little test to see the result of the augmentation \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "index = randint(0,len(X_train))\n",
    "figure()\n",
    "imshow(X_train[index])\n",
    "figure()\n",
    "imshow(augmented_image_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.10 : augmentation\n",
    "# Needs [ CELL 1.8 ] \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Second method of data augmentation \n",
    "# All classes are represented with the same number of sign, the max already in \n",
    "\n",
    "memory = zeros(nbr_class, dtype=int)\n",
    "lim = max(initial_nbr_train) #500\n",
    "counter = 0\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "for i in range(nbr_class) :\n",
    "\n",
    "    memory[i] = lim - initial_nbr_train[i]\n",
    "    counter += lim - initial_nbr_train[i]\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# We create new sets that we'll fill with the data of the initial sets + the augmented data\n",
    "\n",
    "X_train_second = zeros((len(X_train) + counter, resol[0], resol[1], 3))\n",
    "y_train_second = zeros(len(y_train) + counter)\n",
    "\n",
    "X_train_second[:len(X_train)] = X_train.copy()\n",
    "y_train_second[:len(y_train)] = y_train.copy()\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# We'll start adding values at this index\n",
    "\n",
    "index = len(X_train)\n",
    "\n",
    "nbr_train_second = initial_nbr_train.copy() \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "for i in range(nbr_class) :\n",
    "\n",
    "    indices = where(y_train.copy() == i)\n",
    "    augmented_image = gather(X_train.copy(), indices=indices[0])\n",
    "\n",
    "    for j in range(memory[i]) :\n",
    "\n",
    "        idx = randint(0, len(indices[0])-1)\n",
    "\n",
    "        nbr_train_second[i] += 1 \n",
    "\n",
    "        X_train_second[index] = augmented_image[idx]\n",
    "        y_train_second[index] = i\n",
    "        index += 1\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "print(\"Shape of the augmented training set with second method : \", X_train_second.shape)\n",
    "print(\"Shape of the augmented training target with second method : \", y_train_second.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.11 :  visualisation of the dataset\n",
    "# Needs [ CELL 1.10 ]\n",
    "\n",
    "# Visualisation of the augmented sets \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "graphs(nbr_train_second,y_train_second )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1.12 : categorical\n",
    "# Needs [ CELL 1.10 ]\n",
    "\n",
    "# We put our results to categorical\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "y_test_tc = to_categorical(y_test.copy(), nbr_class)\n",
    "y_train_tc = to_categorical(y_train.copy(), nbr_class)\n",
    "\n",
    "print(\"Shape of y_train without augmentation : \", y_train_tc.shape)\n",
    "print(\"Shape of y_test without augmentation : \", y_test_tc.shape)\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "y_train_second_tc = to_categorical(y_train_second.copy(), nbr_class)\n",
    "\n",
    "print(\"Shape of y_train with second method of augmentation : \", y_train_second_tc.shape)\n",
    "print(\"Shape of y_test with second method of augmentation : \", y_test_tc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt \n",
    "\n",
    "def model_builder(hp):\n",
    "\n",
    "  model = Sequential()\n",
    "\n",
    "  # Tune the number of filters for the second Conv2D \n",
    "  # Choose an optimal value from 64-128\n",
    "\n",
    "  hp_k1 = hp.Int('kernel_1', min_value = 2, max_value = 7, step = 1)\n",
    "  hp_f1 = hp.Int('filter_1', min_value = 64, max_value = 128, step = 4)\n",
    "  model.add(Conv2D(kernel_size=(hp_k1,hp_k1),filters=hp_f1, activation='relu', input_shape=X_train_second.shape[1:]))\n",
    "\n",
    "  hp_k2 = hp.Int('kernel_2', min_value = 2, max_value = 5, step = 1) \n",
    "  model.add(Conv2D(kernel_size=(hp_k2,hp_k2),filters=hp_f1, activation='relu'))\n",
    "\n",
    "  hp_p1 = hp.Int('pool_1', min_value = 3, max_value = 5, step = 1)\n",
    "  model.add(MaxPool2D(pool_size=(hp_p1,hp_p1)))\n",
    "\n",
    "  hp_d1 = hp.Float(\"dropout_1\", min_value=0.1, max_value=0.5, default=0.25, step=0.05)\n",
    "  model.add(Dropout(rate=hp_d1))\n",
    "\n",
    "\n",
    "  hp_k3 = hp.Int('kernel_3', min_value = 2, max_value = 4, step = 1)\n",
    "  hp_f2 = hp.Int('filter_2', min_value = 128, max_value = 256, step = 16)\n",
    "  model.add(Conv2D(kernel_size=(hp_k3,hp_k3),filters=hp_f2, activation='relu'))\n",
    "\n",
    "  hp_k4 = hp.Int('kernel_4', min_value = 2, max_value = 3, step = 1) \n",
    "  model.add(Conv2D(kernel_size=(hp_k4,hp_k4),filters=hp_f2, activation='relu'))\n",
    "\n",
    "  hp_p2 = hp.Int('pool_2', min_value = 2, max_value = 3, step = 1)\n",
    "  model.add(MaxPool2D(pool_size=(hp_p2,hp_p2)))\n",
    "  \n",
    "  hp_d2 = hp.Float(\"dropout_2\", min_value=0.1, max_value=0.5, default=0.25, step=0.05)\n",
    "  model.add(Dropout(rate=hp_d2))\n",
    "  \n",
    "\n",
    "  model.add(Flatten())\n",
    "\n",
    "  hp_units = hp.Int('dense', min_value = 62, max_value = 302, step = 20)\n",
    "\n",
    "  model.add(Dense(hp_units, activation = 'relu'))\n",
    "  model.add(Dense(nbr_class, activation = 'softmax'))\n",
    "\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping \n",
    "\n",
    "stop_early = EarlyStopping(baseline=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective = 'val_accuracy', \n",
    "                     max_epochs = 3,\n",
    "                     factor = 3,\n",
    "                     directory = 'my_dir',\n",
    "                     project_name = 'hyper_tuning',\n",
    "                     overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_train_second,y_train_second_tc, epochs = 3, verbose = 1, validation_data = (X_test, y_test_tc), callbacks = [stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\\n \n",
    "The optimal kernel in first Convolutional layer is  {best_hps.get('kernel_1')}.\\n  \n",
    "The optimal filter in first Convolutional layer is  {best_hps.get('filter_1')}.\\n  \n",
    "The optimal kernel in second Convolutional layer is  {best_hps.get('kernel_2')}.\\n\n",
    "The optimal filter in second Convolutional layer is  {best_hps.get('filter_2')}.\\n\n",
    "The optimal number of units in the first densely-connected layer is {best_hps.get('dense')}.\\n .\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
